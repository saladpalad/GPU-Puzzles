{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Puzzles in CUDA C++\n",
    "By Devin Shah - [@devinshah16](https://twitter.com/DevinShah16)\n",
    "\n",
    "Puzzles adapted from [Sasha Rush](http://rush-nlp.com/)\n",
    "\n",
    "GPUs are pretty cool.\n",
    "\n",
    "Make your own copy of this notebook in Colab, turn on GPU mode in the settings (`Runtime / Change runtime type`, then set `Hardware accelerator` to `GPU`), and\n",
    "then get to coding. ***You might get a warning saying that the GPU is not being used, but it is in fact being used. Ignore this warning. If using a free version, be careful of quotas.***\n",
    "\n",
    "\n",
    "Read the [CUDA C++ bindings guide ](https://docs.nvidia.com/cuda/pdf/CUDA_C_Programming_Guide.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/AIS-UCLA/cuda-puzzles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd cuda-puzzles/GPU_puzzlers_exec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure `nvcc` is installed. If it is not, this notebook will not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Puzzle 1 - Vector Add\n",
    "Parallelize this CPU program\n",
    "```\n",
    "for (int i = 0; i < N; i++){\n",
    "  C[i] = A[i] + B[i]\n",
    "}\n",
    "```\n",
    "Implement a kernel that adds together each position of `A` and `B` and stores it in `C`. You have 1 thread per position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile zip_kernel.cu\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void VecAdd(float* A, float* B, float* C) {\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o zip -arch=sm_75 zip_runner.cu zip_kernel.cu\n",
    "!./zip\n",
    "!compute-sanitizer ./zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Puzzle 2 - Vector Add with Guards\n",
    "Parallelize this CPU program\n",
    "```\n",
    "for (int i = 0; i < N; i++){\n",
    "  C[i] = A[i] + B[i]\n",
    "}\n",
    "```\n",
    "The number of threads is now greater than `N`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile zip_guards_kernel.cu\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void VecAdd_Guards(float* A, float* B, float* C, int N){\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o zip_guards -arch=sm_75 zip_runner_guards.cu zip_guards_kernel.cu\n",
    "!./zip_guards\n",
    "!compute-sanitizer ./zip_guards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Puzzle 3 - Broadcast\n",
    "\n",
    "Implement a kernel that adds `A` and `B` and stores it in `C`.\n",
    "Inputs `A` and `B` are vectors. You have more threads than positions.\n",
    "1D indexing doesn't work for 2D arrays in CUDA C++. You can calculate the index from i and j by computing `i * size + j`. There is only 1 thread block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile broadcast_kernel.cu\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void Broadcast(float* A, float* B, float* C, int size) {\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o broadcast -arch=sm_75 broadcast_runner.cu broadcast_kernel.cu\n",
    "!./broadcast\n",
    "!compute-sanitizer ./broadcast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Puzzle 4 - Blocks\n",
    "\n",
    "Implement a kernel that adds 10 to each position of `A` and stores it in `C`.\n",
    "You have fewer threads per block than the size of `A`. (i.e. will need to use multiple blocks to cover the entire size)\n",
    "\n",
    "Use `blockIdx * blockDim + threadIdx`\n",
    "\n",
    "*Tip: A block is a group of threads. The number of threads per block is limited, but we can\n",
    "have many different blocks. Variable `cuda.blockIdx` tells us what block we are in.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile blocks_kernel.cu\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void Blocks(float* A, float* C, float size) {\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o blocks -arch=sm_75 blocks_runner.cu blocks_kernel.cu\n",
    "!./blocks\n",
    "!compute-sanitizer ./blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Puzzle 5 - Blocks 2D\n",
    "\n",
    "Implement the same kernel in 2D.  You have fewer threads per block\n",
    "than the size of `A` in both directions.\n",
    "A is 2D and C is now 2D, will need threads along `.x` and `.y` now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile map2d_block_kernel.cu\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void Map2DBlock(float* A, float* C, int size) {\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o map2d_block -arch=sm_75 map2d_block_runner.cu map2d_block_kernel.cu\n",
    "!./map2d_block\n",
    "!compute-sanitizer ./map2d_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Puzzle 6 - Transpose\n",
    "\n",
    "Implement a kernel that transposes `in` (2D matrix) and writes to `out` \n",
    "`Tip` How do you write index via row-major order vs col-major order?\n",
    "\n",
    "\n",
    "`in` is of shape `MxN`, `out` is of shape `NxM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile transpose_kernel.cu\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void Transpose(float *in, float *out, int M, int N){\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o transpose -arch=sm_75 transpose_runner.cu transpose_kernel.cu\n",
    "!./transpose\n",
    "!compute-sanitizer ./transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Puzzle 7 - Transpose Shared\n",
    "\n",
    "Implement a kernel that transposes `in` (2D matrix) and writes to `out`\n",
    "\n",
    "`Tip 1` Think about the size of your shared memory array, and how you would index it differently from in/out which of `larger` shape\n",
    "\n",
    "`Tip 2` We need to do transposing at 2 levels (within the blocks, shared memory) and across blocks (may need to swap blockIdx)\n",
    "\n",
    "`in` is of shape `MxN`, `out` is of shape `NxM`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile transpose_shared_kernel.cu\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void _transpose_shared(float *in, float *out, int M, int N){\n",
    "  extern __shared__ float arr_shared[];\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o transpose_shared -arch=sm_75 transpose_shared_runner.cu transpose_shared_kernel.cu\n",
    "!./transpose_shared\n",
    "!compute-sanitizer ./transpose_shared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Puzzle 8 - Matrix Multiplication\n",
    "A matmul on CPU can be written as a 3 nested for loop program.\n",
    "```\n",
    "for (int i = 0; i < M; i++){\n",
    "  for (int j = 0; j < N; j++){\n",
    "    for (int k = 0; k < K; k++){\n",
    "      C[i][j] += A[i*K+k] * B[k*N+j]; // C[i][j] += A[i][k] * B[k][j]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "Convert this implementation to CUDA (think about about what axis would be parallelized)\n",
    "\n",
    "`A` is of shape `MxK`, B is of shape `KxN`\n",
    "\n",
    "For this problem `M=N=K=2048`, and the size wil evenly divide by the thread block size of 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile matmul_kernel.cu\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void matmul(float *A, float *B, float *C, int M, int N, int K){\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o matmul -arch=sm_75 matmul_runner.cu matmul_kernel.cu\n",
    "!./matmul\n",
    "!compute-sanitizer ./matmul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Puzzle 9 - Matrix Multiplication with Guards\n",
    "A matmul on CPU can be written as a 3 nested for loop program.\n",
    "```\n",
    "for (int i = 0; i < M; i++){\n",
    "  for (int j = 0; j < N; j++){\n",
    "    for (int k = 0; k < K; k++){\n",
    "      C[i][j] += A[i*K+k] * B[k*N+j]; // C[i][j] += A[i][k] * B[k][j]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "Convert this implementation to CUDA (think about about what axis would be parallelized)\n",
    "\n",
    "`A` is of shape `MxK`, B is of shape `KxN`\n",
    "\n",
    "For this problem `M=1025, N=2048, K = 2048`, and the size will need to round up to spawn enough thread blocks along M (look at matmul_runner.cu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile matmul_guards_kernel.cu\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void matmul_guards(float *A, float *B, float *C, int M, int N, int K){\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o matmul_guards -arch=sm_75 matmul_guards_runner.cu matmul_guards_kernel.cu\n",
    "!./matmul_guards\n",
    "!compute-sanitizer ./matmul_guards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Puzzle 10 - Coalesced Matrix Multiplication\n",
    "A matmul on CPU can be written as a 3 nested for loop program.\n",
    "```\n",
    "for (int i = 0; i < M; i++){\n",
    "  for (int j = 0; j < N; j++){\n",
    "    for (int k = 0; k < K; k++){\n",
    "      C[i][j] += A[i*K+k] * B[k*N+j]; // C[i][j] += A[i][k] * B[k][j]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "Given that\n",
    "\n",
    "`int i = blockIdx.x * blockDim.x + threadIdx.x;`\n",
    "\n",
    "`int j = blockIdx.y * blockDim.y + threadIdx.y;`\n",
    "\n",
    "Does `C[i][j] += A[i*K+k] * B[k*N+j]` achieve coalescing?\n",
    "\n",
    "`Tip`: Remember we always want to map `threadIdx.x` along the contiguous dimension, also look at matmul_coalseced_runner.cu to see the gridDim and blockDim\n",
    "\n",
    "`A` is of shape `MxK`, B is of shape `KxN`\n",
    "\n",
    "For this problem `M=1025, N=2048, K = 2048`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile matmul_coalesced_kernel.cu\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void matmul_coalesced(float *A, float *B, float *C, int M, int N, int K){\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o matmul_coaleseced -arch=sm_75 matmul_coalesced_runner.cu matmul_coalesced_kernel.cu\n",
    "!./matmul_coaleseced\n",
    "!compute-sanitizer ./matmul_coaleseced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Puzzle 11 - Matrix Multiplication w/ Shared Memory\n",
    "A matmul on CPU can be written as a 3 nested for loop program.\n",
    "```\n",
    "for (int i = 0; i < M; i++){\n",
    "  for (int j = 0; j < N; j++){\n",
    "    for (int k = 0; k < K; k++){\n",
    "      C[i][j] += A[i*K+k] * B[k*N+j]; // C[i][j] += A[i][k] * B[k][j]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "Speedup your previous matmul implementations w/ shared memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile matmul_shared_kernel.cu\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void matmul_shared(float *A, float *B, float *C, int M, int N, int K, int BLOCK_M, int BLOCK_N, int BLOCK_K){\n",
    "  extern __shared__ float shared[];\n",
    "  float *As = shared;\n",
    "  float *Bs = shared + BLOCK_M*BLOCK_K;\n",
    "\n",
    "  int j = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  int i = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "\n",
    "  float inner_prod = 0.0f;\n",
    "  for (int k_tile = 0; k_tile < K/BLOCK_K; k_tile++) {\n",
    "    // .y threads are mapped along M dim\n",
    "    // .x threads are mapped along N dim\n",
    "    int a_col = (k_tile*BLOCK_K + threadIdx.x);\n",
    "    int b_row = (k_tile*BLOCK_K + threadIdx.y);\n",
    "    As[threadIdx.y*BLOCK_K+threadIdx.x] = A[i*K + a_col];\n",
    "    Bs[threadIdx.y*BLOCK_N+threadIdx.x] = B[b_row*N + j];\n",
    "    __syncthreads();\n",
    "    for (int kk = 0; kk < BLOCK_K; kk++){\n",
    "      inner_prod += As[threadIdx.y*BLOCK_K+kk] * Bs[kk*BLOCK_K+threadIdx.x];\n",
    "    }\n",
    "    __syncthreads();\n",
    "  }\n",
    "\n",
    "  // 1 thread per output element\n",
    "  if (i < M && j < N){\n",
    "    C[i*N+j] = inner_prod;\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o matmul_shared -arch=sm_75 matmul_shared_runner.cu matmul_shared_kernel.cu\n",
    "!./matmul_shared\n",
    "!compute-sanitizer ./matmul_shared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Puzzle 12 - 1D Indexing\n",
    "\n",
    "Copy a 1D tensor using CUDA threads.\n",
    "\n",
    "**Signature:** `Index1D(float* in, float* out, int d)`\n",
    "\n",
    "- Shape: `(d,)` where d=64\n",
    "- Each thread copies one element\n",
    "- Use `threadIdx.x` to index\n",
    "\n",
    "**Launch:** `<<<1, d>>>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile index1d_kernel.cu\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void Index1D(float* in, float* out, int d) {\n",
    "    // TODO: use threadIdx.x to copy elements\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile index1d_kernel.cu\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void Index1D(float* in, float* out, int d) {\n",
    "    // TODO: use threadIdx.x to copy elements\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Puzzle 13 - 2D Indexing\n",
    "\n",
    "Copy a 2D tensor (n,d) using 2D thread blocks.\n",
    "\n",
    "**Signature:** `Index2D(float* in, float* out, int n, int d)`\n",
    "\n",
    "- Shape: `(n, d)` where n=16, d=64\n",
    "- `threadIdx.y` → rows (n)\n",
    "- `threadIdx.x` → cols (d)\n",
    "- Row-major index: `i*d + j`\n",
    "\n",
    "**Launch:** `<<<1, dim3(d, n)>>>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile index2d_kernel.cu\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void Index2D(float* in, float* out, int n, int d) {\n",
    "    // TODO: use threadIdx.y, threadIdx.x\n",
    "    // Remember: i*d + j for row-major indexing\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile index2d_kernel.cu\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void Index2D(float* in, float* out, int n, int d) {\n",
    "    // TODO: use threadIdx.y, threadIdx.x\n",
    "    // Remember: i*d + j for row-major indexing\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Puzzle 14 - 3D Indexing\n",
    "\n",
    "Copy a 3D tensor (h,n,d) using blocks + 2D threads.\n",
    "\n",
    "**Signature:** `Index3D(float* in, float* out, int h, int n, int d)`\n",
    "\n",
    "- Shape: `(h, n, d)` where h=20, n=16, d=64\n",
    "- `blockIdx.x` → height (h)\n",
    "- `threadIdx.y` → rows (n)\n",
    "- `threadIdx.x` → cols (d)\n",
    "- Index: `i*n*d + j*d + k`\n",
    "\n",
    "**Launch:** `<<<h, dim3(d, n)>>>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile index3d_kernel.cu\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void Index3D(float* in, float* out, int h, int n, int d) {\n",
    "    // TODO: use blockIdx.x, threadIdx.y, threadIdx.x\n",
    "    // Formula: i*n*d + j*d + k\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile index3d_kernel.cu\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void Index3D(float* in, float* out, int h, int n, int d) {\n",
    "    // TODO: use blockIdx.x, threadIdx.y, threadIdx.x\n",
    "    // Formula: i*n*d + j*d + k\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Puzzle 15 - 4D Indexing\n",
    "\n",
    "Copy a 4D tensor (b,h,n,d) - like transformers!\n",
    "\n",
    "**Signature:** `Index4D(float* in, float* out, int b, int h, int n, int d)`\n",
    "\n",
    "- Shape: `(b, h, n, d)` where b=4, h=20, n=16, d=64\n",
    "- `blockIdx.y` → batch (b)\n",
    "- `blockIdx.x` → heads (h)\n",
    "- `threadIdx.y` → seq_len (n)\n",
    "- `threadIdx.x` → dim (d)\n",
    "- Index: `i*h*n*d + j*n*d + k*d + l`\n",
    "\n",
    "**Launch:** `<<<dim3(h, b), dim3(d, n)>>>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile index4d_kernel.cu\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void Index4D(float* in, float* out, int b, int h, int n, int d) {\n",
    "    // TODO: use blockIdx.y, blockIdx.x, threadIdx.y, threadIdx.x\n",
    "    // Formula: i*h*n*d + j*n*d + k*d + l\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile index4d_kernel.cu\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void Index4D(float* in, float* out, int b, int h, int n, int d) {\n",
    "    // TODO: use blockIdx.y, blockIdx.x, threadIdx.y, threadIdx.x\n",
    "    // Formula: i*h*n*d + j*n*d + k*d + l\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
